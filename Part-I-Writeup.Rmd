---
title: "Part-I-Writeup"
author: "Brian Cozzi"
date: "12/7/2018"
output: pdf_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2)
library(glmnet)
library(knitr)
```

```{r read-data, echo=TRUE, message=FALSE}
load("paintings_train.Rdata")
load("paintings_test.Rdata")
set.seed(9)
```


### Part I Write up *Last day to submit is Dec 7 by 5; accepted until Dec 6 (5 points off if late)*

Once you are satisfied with your model, provide a write up of your data analysis project in a new Rmd file/pdf file: `Part-I-Writeup.Rmd` by copying over salient parts of your R notebook. The written assignment consists of five parts:

##1. Introduction: Summary of problem and objectives (5 points)

INTRO PARAGRAPH
The art historian would like to know what factors drove prices of painting, which paintings might be overvalued and which are undervalued.   It is up to you to decide what methods you want to use (frequentist or Bayesian or a combination) to answer these questions, and implement them to help to identify undervalued and overvalued paintings, as well as which features and possible interactions are at play.

Initial inspection of the data revealed that it contained 59 variables: 2 outcomes (price and logprice) and 57 predictors. These predictors fit into several relatively well-defined categories: \
- Artist Characteristics \
- Auction Characteristics \
- Painting Content and Physical Features  \

One of the challenges that these data present is that all of these features were categorical variables with varying levels of sparsity. That is, we may have very precise definitions of the characteristics surrounding the painting, however many of these values are sparsely populated and contain categories that are not well-populated. Therefore, one of our goals in the next seciton will be to identify these predictors and adjust them so they can be interpreted intuitively. 

Another major challenge is the redundancy of some variables. For instance, the variables describing the paintings physical features (height, diameter, surface, shape) can be reduced considerably. Therefore, another goal of subsequent sections will be to further reduce these data to improve interpretability and reduce the number of features that need to be used in the model. 


##2. Exploratory data analysis (10 points): must include three correctly labeled graphs and an explanation that highlight the most important features that went into your model building.
  - log transform of surface
  - box-tidwell for other variables
  
  Transform variables initially
  OLS - describe variable transformations
  - stepwise - which variables get selected here when we include the interaction
  
  Data cleaning - 
  - variables with a lot of factors, difficult to interpret
  - missing values. We will work to impute
  
Many variables -> we are working towards interpretability
  - 3D plot to check for interactions
  - pairs of variables
  
  
  BMA - find the highest probability model and best predictive model
  
```{r Physical Features, message=FALSE}
# Shape and Surface
plot1 = ggplot(data = paintings_train, 
               aes(x = Surface, y = logprice,
                   shape = school_pntg, color = Shape))+
                geom_point() + ggtitle("Surface and Log Price")

plot2 = ggplot (data = paintings_train, 
                aes(x = log(Surface), y = logprice,
                    shape = school_pntg, color = Shape))+
                geom_point() + ggtitle("Log Surface and Log Price") 
                
plot.df = paintings_train %>% mutate(lot = str_replace_all(lot, "lot", "")%>% as.numeric())
plot3 = ggplot (data = plot.df, 
                aes(x = lot, y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("Lot and Log Price")
plot4 = ggplot (data = plot.df, 
                aes(x = log(lot), y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("Log Lot and Log Price")

gridExtra::grid.arrange(plot1, plot2, plot3, plot4, nrow = 2)
```

We can see from this that 
1) log transform is definitely necessary. When it's transformed we start to see a more linear trend. 
2) We also see that some of the dealers 

```{r Few Tables}
ggplot (data = paintings_train, 
                aes(x = school_pntg, y = logprice, color = factor(artistliving)))+
                geom_jitter() + ggtitle("Artist Features and Log Price") 
```
Artist information include 6 variables, "school_pntg", "subject",     "authorstandard", "artistliving", "authorstyle", and "author". "subject" is text description of the painting, which is very hard to quantify or act as factors, so we decide to delete it. "authorstandard" is the standard version of the "author". "authorstandard" has 519 unique values, while "author" has 831 unique values. So we think using "authorstandard" instead of "author" would be a better choice. 

In summary, we would use four variables under this category, "school_pntg", "artistliving", "authorstandard", and "authorstyle". And we would use all of them as factors. For "school_pntg", we have 4 levels by combining "A|G|S|X" into one category. We combine those four levels because their data sizes are very small. For "artistliving", it is a binary variable with 0/1. For "authorstandard", we pick authors, whose corresponding mean value of logprices is at least 10% bigger than the mean value of all logprices in the sample data, or at least 10% lower than that, as 1. The rest authors are set as 0. For "authorstyle", we have 5 levels by combining "m|g|o|al|co|pa" since their sample sizes are very small.

From the plot, we can see that "authorstandard" and "school_pntg" may have impact on logprice. "Artistliving" show pretty sparse values of 1 (living), thus making it more difficult to determine a relationship.

Looking at the plots for binary (integer) predictors, we are looking to identify whether a value of 1 or 0 for a variable changes how it is distributed across logprice. But the plots don't look obvious enough to tell the relationships.


## Include chart by year and material type
```{r}
ggplot (data = paintings_train, 
                aes(x = year, y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("Artist Features and Log Price") 



```

##3. Development and assessment of an initial model (10 points)

* Initial model: must include a summary table and an explanation/discussion for variable selection and overall amount of variation explained. 
```{r Format Data}
# Formatting for some variables
# Top 10 bidders
data = paintings_train
HighRollers = data %>% group_by(winningbidder) %>% 
              summarize(count = n(), avg.spend = mean(logprice)/mean(data$logprice)) %>% 
              filter(count>10) %>% arrange(desc(avg.spend)) %>% head(10) %>%
              select(winningbidder) %>%
              head(10) %>% as.list

# Top 5 Pricey Painters
BigShotPainters = data %>% group_by(author) %>% 
              summarize(count = n(), avg.haul = mean(logprice)/mean(data$logprice)) %>% 
              filter(count>10) %>% arrange(desc(avg.haul)) %>%
              select(author) %>%
              head(5) %>% as.list

## Train data
data.train = paintings_train %>% 
      mutate( # Painting physical features
           Shape = ifelse(Shape=="squ_rect", 1, 0),
           log.Surface = log(Surface),
           Height_in = coalesce(Height_in, Diam_in),
           Width_in = coalesce(Width_in, Diam_in),
           year = factor(year),
           Nfigures = ifelse(nfigures>1, 1, 0),
           Material = material %in% c("cuivre", "argent"),
           # Sale Features
           position = ifelse(position>1, 1, position),
           # Buyer Characteristics
           High_Roller = ifelse(winningbidder %in% 
                                  unlist(HighRollers), 1, 0),
           BigShotPainter = ifelse(author %in% 
                                  unlist(BigShotPainters), 1, 0),
           endbuyer = ifelse(endbuyer=="U"|winningbiddertype == "",
                                      "None",
                                      winningbiddertype) %>% as.factor(), 
           dealer = factor(dealer),
           lot = str_replace_all(lot, "lot", "")%>% as.numeric(),
           origin_author = factor(origin_author),
           winningbiddertype = ifelse(is.na(winningbiddertype)|winningbiddertype == "",
                                      "None",
                                      winningbiddertype) %>% as.factor(),
           type_intermed = factor(ifelse(type_intermed=="", 
                                         "None",
                                         type_intermed)),
           school_pntg=as.factor(school_pntg),
           authorstandard=as.factor(authorstandard),
           artistliving=as.factor(artistliving),
           authorstyle=as.factor(authorstyle)
) 

sum(is.na((data$lot %>% str_replace_all(., "lot", "") %>% as.numeric())))

#variable description tables
table=data.train %>% group_by(school_pntg) %>% 
  summarize(count = n(), 
            avg.spend = mean(logprice)/mean(data.train$logprice)) %>% 
  arrange(desc(avg.spend))

table1=data.train %>% group_by(authorstandard) %>% 
  summarize(count = n(), avg.spend = mean(logprice)/mean(data.train$logprice)) %>% 
  filter(abs(avg.spend-1)>0.1) %>% 
  arrange(desc(avg.spend))

table2=data.train %>% group_by(authorstyle) %>% 
  summarize(count = n(), avg.spend = mean(logprice)/mean(data.train$logprice)) %>% 
  arrange(desc(avg.spend))

table3=data.train %>% group_by(artistliving) %>% 
  summarize(count = n(), avg.spend = mean(logprice)/mean(data.train$logprice)) %>% 
  arrange(desc(avg.spend))
#reduce factor levels for three varaibles
levels(data.train$school_pntg) <- c(levels(data.train$school_pntg), "other or unknown") 
data.train$school_pntg[data$school_pntg %in% c("A","S","G","X")] <- "other or unknown" 
data.train$authorstandard<- ifelse(data.train$authorstandard  %in% table1$authorstandard, 1, 0)
levels(data.train$authorstyle) <- c(levels(data.train$authorstyle), "other","missing") 
data.train$authorstyle[data.train$authorstyle %in% c("m","g","o","al","co","pa")] <- "other" 
data.train$authorstyle[data.train$authorstyle %in% c("")] <- "missing" 

# Selecting all the relevant variables for final table
data.train = data.train%>% 
  select(logprice, Shape, log.Surface, Height_in, Width_in, year, artistliving,
         authorstyle, diff_origin, lot,
          High_Roller, BigShotPainter, Nfigures, Material, position, endbuyer, 
          origin_author, winningbiddertype, type_intermed, dealer, authorstyle,
          authorstandard, school_pntg, dealer, origin_author, winningbiddertype, 
          engraved, original, prevcoll, othartist, paired, figures, diff_origin,
          lrgfont, relig, landsALL, lands_sc, lands_elem, lands_figs, lands_ment, 
          arch, mytho, peasant, othgenre, singlefig, portrait, still_life, discauth, 
          history, allegory, pastorale, other) %>% na.omit()

# train = paintings_train %>% select(sale, lot, position, dealer # lot was excluded (too many vars), sale was excluded, 
# ,year, origin_author, origin_cat, school_pntg 
# ,diff_origin, logprice 
# ,subject, authorstandard, artistliving, authorstyle # Dropped subject. too specific
# ,author, winningbidder, winningbiddertype, endbuyer # Changed author and winning bidder to top names 
# ,Interm, type_intermed, # dropped Interm
# Height_in, Width_in, Diam_in # Reduced to just Height/Width
# ,Surface_Rect, Surface_Rnd, Shape ,Surface, # Reduced to 2
# 
# material, mat, materialCat #- reduced this to 1 variable
# ,nfigures, engraved, original, prevcoll 
# ,othartist, paired, figures, finished 
# ,lrgfont, relig, landsALL, lands_sc 
# ,lands_elem, lands_figs, lands_ment, arch ,
# mytho, peasant, othgenre, singlefig, 
# portrait, still_life, discauth, history, 
# allegory, pastorale, other 
# )

# Are these last categories super sparse?

test = data.train %>% select( prevcoll
,othartist, paired, figures
,lrgfont, relig, landsALL, lands_sc
,lands_elem, lands_figs, lands_ment, arch ,
mytho, peasant, othgenre, singlefig,
portrait, still_life, discauth, history,
allegory, pastorale, other
)

for (i in 1:ncol(test)) print(paste0(colnames(test)[i], ":", round(mean(test[,i], na.rm = TRUE), 2)))


```


* Model selection: must include a discussion
```{r BIC}
model1 = lm(logprice ~ . +log(lot) + lot:dealer + log.Surface:Shape + type_intermed:endbuyer + dealer:year -winningbiddertype, data.train)

### BMA - selecting the highest probability model
#AIC
#25 variables
model2 = step(model1,direction="both",k=2, trace =FALSE)
summary(model2)
#BIC
# 16 variables
model3 = step(model1,direction="both",k=log(nrow(data.train)), trace=FALSE)
summary(model3)
#plot coefficient comparison model btw aic, bic, and ols
 beta_bic_aic_ols = data_frame(name=names(coef(model1)),OLS=coef(model1))%>%
  left_join(data_frame(name=names(coef(model2)),AIC=coef(model2)))%>%
  left_join(data_frame(name=names(coef(model3)),BIC=coef(model3)))
kable(beta_bic_aic_ols)
```
As is shown in the table, 22 variables are selected by AIC, while 14 variables are selected by BIC. BIC excluded all variables that are excluded by AIC, except for "Interm", which is excluded by AIC but included in BIC model.
Under AIC model, variables including get significant result under 95% confidence level. 

```{r BAS warning = FALSE, message = FALSE}
library(BAS)
basg=bas.lm(logprice ~ . + log(lot) + lot:dealer + log.Surface:Shape + type_intermed:endbuyer + dealer:year -winningbiddertype -authorstyle -school_pntg
, data=data.train,
prior="g-prior", a=nrow(data.train), modelprior=uniform(),
method="MCMC", MCMC.iterations = 500000, thin = 20)
plot(basg)
diagnostics(basg, type="pip")

#pred=predict(baslm, newdata = df, estimator='HPM')
basz = bas.lm(logprice ~ .+ log(lot) + lot:dealer + log.Surface:Shape + type_intermed:endbuyer + dealer:year -winningbiddertype -authorstyle -school_pntg, 
               data=data.train,
               prior="JZS", a=nrow(data.train), modelprior=uniform(),
               method="MCMC", MCMC.iterations = 500000, thin = 20, 
               initprobs="marg-eplogp")
plot(basz)
diagnostics(basz, type="pip")
image(basg, rotate = F)
image(basz, rotate = F)



bas.summary <- summary(paint.bas)
kable(bas.summary, align = "c", digits =3 )
diagnostics(paint.bas, type = "pip")


image(paint.bas)
plot(paint.bas, which = 4)
```


```{r LASSO}
RMSE = function(obs, pred){
  return( mean(sqrt((obs-pred)^2)))}

In.Interval = function(upper, lower, observed){
  mean(observed>=lower && observed<=upper)
}

### LASSO
y = data.train[,1]
x = model.matrix(logprice ~., data = data.train)
lasso = cv.glmnet(y = y, 
                  x = x,
                  standardize=TRUE,
                  alpha = 1)
plot(lasso)
bestlambda = lasso$lambda.min
lasso.pred=predict(lasso,
                   newx=x,
                   s=bestlambda)  
# Creating a table of the selected variables
Selected.Variables = as.data.frame((coef(lasso, bestlambda))
                                   [which(coef(lasso, bestlambda)!=0),])
colnames(Selected.Variables) = "Value"
kable(Selected.Variables, align = 'c', digits = 4, 
      format = 'markdown', 
      caption = "Selected Variables from LASSO")
lasso.RMSE = RMSE(lasso.pred, y)
## Bootstrapping for prediction interval 
boots = 50
preds = matrix(nrow = nrow(x), ncol = boots)
for (i in 1:boots){
  samp = sample(nrow(x), replace = TRUE)
  x = x[samp,]
  y = y[samp]
  lasso = cv.glmnet(y = y, x = x,
                       standardize=TRUE,
                       alpha = 1)
  preds[,i] = predict(lasso, x, s = "lambda.min")
}
Var.Pred = apply(preds, 1, var)
SE = lasso.RMSE + sqrt(Var.Pred)
lasso.predictions = data_frame(fit = as.vector(lasso.pred), actual = as.vector(y) , 
                               lower = as.vector(lasso.pred - 1.96*SE),
                               upper = as.vector(lasso.pred + 1.96*SE))
ggplot(data = lasso.predictions %>% arrange(fit), aes(x = actual, y = fit)) + 
        geom_point() + 
        geom_errorbar(aes(ymin = lower, ymax = upper)) + 
        geom_abline(intercept = 0, slope = 1)
lasso.coverage = In.Interval(lower = lasso.predictions$lower, 
                             upper = lasso.predictions$upper, 
                             observed = lasso.predictions$actual)

```



* Residual: must include residual plot(s) and a discussion.  
```{r}


```


* Variables: must include table of coefficients and CI
```{r Table of Variables and CI}


```





##4. Summary and Conclusions (10 points)

What is the (median) price for the "baseline" category if there are categorical or dummy variables in the model (add CI's)?  (be sure to include units!) Highlight important findings and potential limitations of your model.  Does it appear that interactions are important?  What are the most important variables and/or interactions?  Provide interprations of how the most important variables influence the (median) price giving a range (CI).  Correct interpretation of coefficients for the log model desirable for full points.

Provide recommendations for the art historian about features or combination of features to look for to find the most valuable paintings.

_Points will be deducted for code chunks that should not be included, etc._

*Upload write up  to Sakai any time before Dec 7th*

###  Evaluation on test data for Part I

Once your write up is submitted, your models will be evaluated on the following criteria based on predictions  on the test data (20 points): 

* Bias:  Average (Yhat-Y)  positive values indicate the model tends to overestimate price (on average) while negative values indicate the model tends to underestimate price.
* Maximum Deviation:  Max |Y-Yhat| -  identifies the worst prediction  made in the validation data set.
* Mean Absolute Deviation:  Average |Y-Yhat| - the average error (regardless of sign).
* Root Mean Square Error: Sqrt Average (Y-Yhat)^2
* Coverage:  Average( lwr < Y < upr) 

In order to have a passing wercker badge, your file for predictions needs to be the same length as the test data, with three columns:  fitted values, lower CI and upper CI values in that order with names, *fit*, *lwr*, and *upr* respectively such as in the code chunk below. 

Save predictions and intervals.  
```{r predict-model-final, echo=FALSE, include=FALSE}
# change model1 or update as needed
# predictions = as.data.frame(
#   exp(predict(model1, newdata=paintings_test,
#               interval = "pred")))
# save(predictions, file="predict-test.Rdata")
```


You will be able to see your scores on the score board.  They will be initialized by a prediction based on the mean in the training data.


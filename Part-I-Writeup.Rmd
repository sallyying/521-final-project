---
title: "Part-I-Writeup"
author: "Brian Cozzi, Zhaolin Ying, Wei Zhang"
date: "12/7/2018"
output: pdf_document
---

```{r setup, include=FALSE, message=FALSE}
knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2) 
library(glmnet)
library(knitr)
```

```{r read-data, echo=TRUE, message=FALSE}
load("paintings_train.Rdata")
load("paintings_test.Rdata")
set.seed(9)
```


### Part I Write up *Last day to submit is Dec 7 by 5; accepted until Dec 6 (5 points off if late)*

##1. Introduction: Summary of problem and objectives (5 points)

We are tasked with identifying the relevant variables for predicting the cost of a painting for a few years in the 18th century. In addition to gaining insight on these variables, we are also interested in generating a prediction to identify paintings that may have been overvalued or undervalued given a set of predictors. There are many factors that drive the prices of paintings. Our team wants to find the intrinsic value of paintings then we can judge whether each specific painting on the market has been overvalued or undervalued. In our project, we focus on the variable selection and tries to detect which variables tend to be informative.


Initial inspection of the data revealed that it contained 59 variables: 2 outcomes (price and logprice) and 56 predictors. These predictors fit into several relatively well-defined categories: \
- Artist Characteristics \
- Auction Characteristics \
- Painting Content and Physical Features  \

One of the challenges that these data present is that all of these features were categorical variables with varying levels of sparsity. That is, we may have very precise definitions of the characteristics surrounding the painting, however many of these values are sparsely populated and contain categories that are not well-populated. Therefore, one of our goals in the next seciton will be to identify these predictors and adjust them so they can be interpreted intuitively. 

Another major challenge is the redundancy of some variables. For instance, the variables describing the paintings physical features (height, diameter, surface, shape) can be reduced considerably. Therefore, another goal of subsequent sections will be to further reduce these data to improve interpretability and reduce the number of features that need to be used in the model. 


##2. Exploratory data analysis (10 points): must include three correctly labeled graphs and an explanation that highlight the most important features that went into your model building.

#### Physical Features of Painting
```{r Physical Features, message=FALSE, echo=FALSE}
# Shape and Surface
plot1 = ggplot(data = paintings_train, 
               aes(x = Surface, y = logprice,
                   shape = school_pntg, color = Shape))+
                geom_point() + ggtitle("Surface and Log Price")

plot2 = ggplot (data = paintings_train, 
                aes(x = log(Surface), y = logprice,
                    shape = school_pntg, color = Shape))+
                geom_point() + ggtitle("Log Surface and Log Price") 
                
plot.df = paintings_train %>% mutate(lot = str_replace_all(lot, "lot", "")%>% as.numeric())
plot3 = ggplot (data = plot.df, 
                aes(x = lot, y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("Lot and Log Price")
plot4 = ggplot (data = plot.df, 
                aes(x = log(lot), y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("Log Lot and Log Price")

plot5 = ggplot (data = paintings_train, 
                aes(x = nfigures, y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("NFigures and Log Price")
plot6 = ggplot (data = plot.df, 
                aes(x = log(nfigures+1), y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("Log NFigures and Log Price")

gridExtra::grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, nrow = 3)


```

The plots above show several variables that become increasingly sparse as the values increase. In other words, their distributions are very positively skewed making them good candidates for a log transformation. The plots on the left show the relationship between the untransformed variables on the x axis and log price on the y axis and the plots on the right show the relationship of the log transformed values. Clearly, in all cases, the log transformation makes these plots more readable, though it seems that the only clear linear relationship is with log surface. For log lot, there does not appear to be a discernable relationship with price. With NFigures, we notice there are a lot of . Because these observations could significantly impact the intercept of this relationship, we determined that it would be best to simply transform this into a dichotomous variable with 0 signifiying 0 or 1 figures and 1 signifying more than 1 figures. 

We also see from these plots that the dealer, regardless of the variable on the x axis, seems to stratify the price of the painting fairly neatly. Additionally, we can see from the plot with surface that nearly all the paintings are squares or rectangles. 

#### Painter Characteristics
```{r Few Tables, echo=FALSE}
ggplot (data = paintings_train, 
                aes(x = school_pntg, y = logprice, color = factor(artistliving)))+
                geom_jitter() + ggtitle("Artist Features and Log Price")

```
Artist information includes 6 variables, "school_pntg", "subject",     "authorstandard", "artistliving", "authorstyle", and "author". "subject" is text description of the painting, which is very hard to quantify or act as factors, so we decide to delete it. "authorstandard" is the standard version of the "author". "authorstandard" has 519 unique values, while "author" has 831 unique values. So we think using "authorstandard" instead of "author" would be a better choice. 

In summary, we would use four variables under this category, "school_pntg", "artistliving", "authorstandard", and "authorstyle". And we would use all of them as factors. For "school_pntg", we have 4 levels by combining "A|G|S|X" into one category. We combine those four levels because their data sizes are very small. For "artistliving", it is a binary variable with 0/1. For "authorstandard", we pick authors, whose corresponding mean value of logprices is at least 10% bigger than the mean value of all logprices in the sample data, or at least 10% lower than that, as 1. The rest authors are set as 0. For "authorstyle", we have 5 levels by combining "m|g|o|al|co|pa" since their sample sizes are very small.

From the plot, we can see that "authorstandard" and "school_pntg" may have impact on logprice. "Artistliving" show pretty sparse values of 1 (living), thus making it more difficult to determine a relationship.

Looking at the plots for binary (integer) predictors, we are looking to identify whether a value of 1 or 0 for a variable changes how it is distributed across logprice. But the plots don't look obvious enough to tell the relationships.


#### Yearly Effects
```{r Sales by Dealer and Year, echo=FALSE}

ggplot(data = paintings_train, aes(x = factor(year), y = logprice, color = dealer)) + 
  geom_jitter() + ggtitle("Sales by Dealer and Year") + xlab("Year")

```
The visualization above shows that the effect of hear on the median sale price does not appear to be linear. Without taking dealer into account yet, it seems that some years have very little activity and others are densely populated. From this visualization, it seems clear year should be interpreted as a factor given the information we have at our disposal. It is worth considering that the relative activity or inactivity may be due to other economic factors for which we do not have data. 

Another key takeaway from this is that some dealers were particularly active for some years, but relatively inactive for others. Without much subject area expertise, it is difficult to develop some intuition for why this may be happening. While this may initially suggest that an interaction term is useful, it should be noted that there are some years in which dealers are totally inactive which could seriously hinder the interpretability of the coefficients for the inactive years. 


##3. Development and assessment of an initial model (10 points)

* Initial model: must include a summary table and an explanation/discussion for variable selection and overall amount of variation explained. 
```{r Format Data, echo=FALSE}
# Formatting for some variables
# Top 10 bidders
data = paintings_train
HighRollers = data %>% group_by(winningbidder) %>% 
              summarize(count = n(), avg.spend = mean(logprice)/mean(data$logprice)) %>% 
              filter(count>10) %>% arrange(desc(avg.spend)) %>% head(10) %>%
              select(winningbidder) %>%
              head(10) %>% as.list

# Top 5 Pricey Painters
BigShotPainters = data %>% group_by(author) %>% 
              summarize(count = n(), avg.haul = mean(logprice)/mean(data$logprice)) %>% 
              filter(count>10) %>% arrange(desc(avg.haul)) %>%
              select(author) %>%
              head(5) %>% as.list

## Train data
data.train = paintings_train %>% 
      mutate( # Painting physical features
           Shape = as.factor(ifelse(Shape=="squ_rect", 1, 0)),
           log.Surface = log(Surface),
           Height_in = coalesce(Height_in, Diam_in),
           Width_in = coalesce(Width_in, Diam_in),
           year = factor(year),
           Nfigures = as.factor(ifelse(nfigures>1, 1, 0)),
           Material = as.factor(material %in% c("cuivre", "argent")),
           position = as.factor(ifelse(position>1, 1, position)),
           # Buyer Characteristics
           High_Roller =as.factor(ifelse(winningbidder %in% 
                                  unlist(HighRollers), 1, 0)),
           BigShotPainter = as.factor(ifelse(author %in% 
                                  unlist(BigShotPainters), 1, 0)),
           endbuyer = ifelse(endbuyer=="U"|winningbiddertype == "",
                                      "N",
                                      winningbiddertype) %>% 
                                  substr(.,1,1) %>% as.factor(), 
           dealer = factor(dealer),
           lot = str_replace_all(lot, "lot", "")%>% as.numeric(),
           origin_author = factor(origin_author),
           winningbiddertype = ifelse(is.na(winningbiddertype)|winningbiddertype == "",
                                      "None",
                                      winningbiddertype) %>% as.factor(),
           type_intermed = as.factor(ifelse(type_intermed=="", 
                                         "None",
                                         type_intermed)%>% substr(.,1,1)),
           school_pntg= ifelse(school_pntg %in% c("A","S","G","X"),
                               "other or unknown", school_pntg) %>% 
                                as.factor(),
           authorstandard=as.factor(authorstandard),
           artistliving=as.factor(artistliving),
           authorstyle=ifelse(authorstyle %in% c("m","g","o","al","co","pa"), 
                              "other", ifelse(authorstyle=="", "missing", 
                                              authorstyle)) %>% as.factor()
      )
cols <- c(names((data.train)[sapply(data.train, is.integer)]))
data.train[,cols] <- lapply(data.train[,cols], factor)

sum(is.na((data$lot %>% str_replace_all(., "lot", "") %>% as.numeric())))

#variable description tables
table=data.train %>% group_by(school_pntg) %>% 
  summarize(count = n(), 
            avg.spend = mean(logprice)/mean(data.train$logprice)) %>% 
  arrange(desc(avg.spend))

table1=data.train %>% group_by(authorstandard) %>% 
  summarize(count = n(), avg.spend = mean(logprice)/mean(data.train$logprice)) %>% 
  filter(abs(avg.spend-1)>0.1) %>% 
  arrange(desc(avg.spend))

table2=data.train %>% group_by(authorstyle) %>% 
  summarize(count = n(), avg.spend = mean(logprice)/mean(data.train$logprice)) %>% 
  arrange(desc(avg.spend))

table3=data.train %>% group_by(artistliving) %>% 
  summarize(count = n(), avg.spend = mean(logprice)/mean(data.train$logprice)) %>% 
  arrange(desc(avg.spend))
#reduce factor levels for three varaibles
data.train$school_pntg[data$school_pntg %in% c("A","S","G","X")] <- "other or unknown" %>% as.factor
data.train$authorstandard<- ifelse(data.train$authorstandard  %in% table1$authorstandard, 1, 0)
# levels(data.train$authorstyle) <- c(levels(data.train$authorstyle), "other","missing") 
# data.train$authorstyle[data.train$authorstyle %in% c("m","g","o","al","co","pa")] <- "other" 
# data.train$authorstyle[data.train$authorstyle %in% c("")] <- "missing" 

# Selecting all the relevant variables for final table
data.train = data.train%>% 
  select(logprice, Shape, log.Surface, Height_in, Width_in, year, artistliving,
         diff_origin, lot,
          High_Roller, BigShotPainter, Nfigures, Material, position, endbuyer, 
          origin_author, type_intermed,
          authorstandard, school_pntg, dealer, origin_author, 
          engraved, original, prevcoll, othartist, paired, figures, diff_origin,
          lrgfont, relig, landsALL, lands_sc, lands_elem, lands_figs, lands_ment, 
          arch, mytho, peasant, othgenre, singlefig, portrait, still_life, discauth, 
          history, allegory, pastorale, other) %>% na.omit()

```
One of the biggest challenges of cleaning this data was determining how to retain relevant information while ensuring there were no groups of factors that were too sparsely populated. In that case, the estimates for those sparsely populated variables could lead to serious problems with model builiding and could adversely impact the model's ability to generalize. Some of these variables that had to be manipulated included material, shape, type of intermediary and winning bidder. There were also several variables that we changed to dichotomous because the factors were too sparse and the impact of the additional categories were approximately the same as others. 


* Model selection: must include a discussion

In order to reduce the variables to only those that had a significant impact on cost, we followed a 2-step approach. 


### Model Selection: BIC

Step 1 involved running 2 stepwise BIC regressions on logprice using 2 different sets of variables and all possible 2 way interactions for each set. The first set of variables included the painting's physical characteristics such as size, shape, number of figures and material. The second set of variables used terms that were related to the bid such as HighRoller, type_intermed and position. The resulting interaction terms were used in the final BAS model where running a two-way interaction for every variable in our training dataset was infeasible. 

```{r BIC, warning = FALSE, message = FALSE, echo=FALSE}
# Physical characteristics
model1.physical = lm(logprice ~ (Shape + log.Surface+ Height_in+ Width_in+ artistliving+
          + BigShotPainter+ Nfigures+ Material+ position+ 
          origin_author + school_pntg + engraved+ original+
           othartist+ paired+ figures)^2, data.train)
set.seed(9)
model3.physical = step(model1.physical,k=log(nrow(data.train)))
summary(model3.physical)
# Artist LivingxBigShotPainter + ArtistLivingxPosition

model1.bidding = lm(logprice ~ (diff_origin+ lot+ High_Roller+ position+ endbuyer+ 
          type_intermed + dealer + prevcoll+
            lrgfont)^2, data.train)
model3.bidding = step(model1.bidding,k=log(nrow(data.train)))
summary(model3.bidding)
# PositionxDealer + LotxDealer

```


### Model Selection: BAS

The BIC stepwise regressions in the first step gave 4 fairly intuitive interactions that were used as predictors in this step- lot:dealer, artistliving:BigShotPainter, position:dealer, artistliving:position. Interestingly, in neither the Best Predictive Model (BPM) nor the Highest Probability Model (HPM) did we observe the interaction terms from the previous step. However, this iteration yielded exactly 20 unique variables for the final model. 

```{r BAS warning = FALSE, message = FALSE, echo=FALSE}
library(BAS)
set.seed(9)
# PositionxDealer + LotxDealer
# Artist LivingxBigShotPainter + ArtistLivingxPosition

basg=bas.lm(logprice ~ . + lot:dealer + artistliving:BigShotPainter + 
              position:dealer + artistliving:position
, data=data.train,
prior="g-prior", a=nrow(data.train), modelprior=uniform(),
method="MCMC", MCMC.iterations = 500000, thin = 20)


par(mfrow = c(2,2))
plot(basg)

## Plotting Confidence interval
plot(confint(coef(basg, estimator = "HPM")))

BPM <- predict(basg, estimator = "BPM")

x = confint(coef(basg, estimator = "BPM"))

Plot.df = data_frame(Variable = attr(x, "dimnames")[[1]], 
                  Estimate = x[,3], Lower = x[,1], 
                  Upper = x[,2]) %>% 
  mutate(In.Zero = ifelse(Upper*Lower<0, 1, 0)) %>%
  filter(Variable %in% variable.names(BPM))

ggplot(data = Plot.df, aes(x = Variable, y = Estimate, color = factor(In.Zero)))+ 
  geom_point() +
  geom_errorbar(aes(ymin = Lower, ymax = Upper))+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1))



# image(basg)


```


* Residual: must include residual plot(s) and a discussion.  
```{r BAS Residual Plot, echo=FALSE}
plot(basg, ask = F,which=1)
```
From the residual plot above, we can see that our the BMA model fits the data well. We have a constant residual with mean zero for different fitted prices. 


* Variables: must include table of coefficients and CI
```{r Table of Variables and CI, echo=FALSE}
## Plotting Confidence interval
x = exp(confint(coef(basg, estimator = "BPM")))
Variables = data_frame(variable = rownames(x), lwr = x[,1], Est = x[,3], upr = x[,2]) %>% 
  filter(variable %in% variable.names(BPM))

kable(Variables, digits = 2, align = 'c')

```






##4. Summary and Conclusions (10 points)

What is the (median) price for the "baseline" category if there are categorical or dummy variables in the model (add CI's)?  (be sure to include units!) Highlight important findings and potential limitations of your model.  Does it appear that interactions are important?  What are the most important variables and/or interactions?  Provide interprations of how the most important variables influence the (median) price giving a range (CI).  Correct interpretation of coefficients for the log model desirable for full points.

Provide recommendations for the art historian about features or combination of features to look for to find the most valuable paintings.

From the exponentiated table of variables above, we can see that the "baseline" category has a median 144.61 Livres. Baseline in this case refers to the situation in which we are only looking at the term for intercept and, because of the nature of the data, the year of sale. It is worth noting that several of the year indicator variables are missing from this model and therefore, we can interpret 1764 and 1768 to be approximately equal when accounting for all other variables. 

One thing that should be noted is that because the model predicts log price, these effects in the table should not be interpreted as additive. In other words, we cannot say, as we would with a traditional OLS regression, that a 1 unit increase in variable $p$ leads to a $\beta_p$ increase/decrease in the outcome variable. nterpreting the log surface and interpreting the non-logged variables


Important findings

Limitations  Extrapolation is extremely difficult because many of these predictors are categorical - a lot of these are categorical and some of these categories are sparsely populated
- year variable - cannot extrapolate by year... they are very different
- regional variable





_Points will be deducted for code chunks that should not be included, etc._

*Upload write up  to Sakai any time before Dec 7th*

###  Evaluation on test data for Part I


Save predictions and intervals.  
```{r predict-model-final, echo=FALSE, include=FALSE}
# change model1 or update as needed
## Test data
data.test = paintings_test %>% 
      mutate( # Painting physical features
           Shape = ifelse(Shape=="squ_rect", 1, 0),
           log.Surface = log(Surface),
           Height_in = coalesce(Height_in, Diam_in),
           Width_in = coalesce(Width_in, Diam_in),
           year = factor(year),
           Nfigures = ifelse(nfigures>1, 1, 0),
           Material = material %in% c("cuivre", "argent"),
           position = ifelse(position>1, 1, position),
           # Buyer Characteristics
           High_Roller = ifelse(winningbidder %in% 
                                  unlist(HighRollers), 1, 0),
           BigShotPainter = ifelse(author %in% 
                                  unlist(BigShotPainters), 1, 0),
           endbuyer = ifelse(endbuyer=="U"|winningbiddertype == "",
                                      "N",
                                      winningbiddertype) %>% 
                                  substr(.,1,1) %>% as.factor(), 
           dealer = factor(dealer),
           lot = str_replace_all(lot, "lot", "")%>% as.numeric(),
           origin_author = factor(origin_author),
           winningbiddertype = ifelse(is.na(winningbiddertype)|winningbiddertype == "",
                                      "None",
                                      winningbiddertype) %>% as.factor(),
           type_intermed = factor(ifelse(type_intermed=="", 
                                         "None",
                                         type_intermed))%>% substr(.,1,1),
           school_pntg= ifelse(school_pntg %in% c("A","S","G","X"),
                               "other or unknown", school_pntg) %>% 
                                as.factor(),
           authorstandard=as.factor(authorstandard),
           artistliving=as.factor(artistliving),
           authorstyle=ifelse(authorstyle %in% c("m","g","o","al","co","pa"), 
                              "other", ifelse(authorstyle=="", "missing", 
                                              authorstyle)) %>% as.factor()
) 

sum(is.na((data$lot %>% str_replace_all(., "lot", "") %>% as.numeric())))

#variable description tables
table=data.test %>% group_by(school_pntg) %>% 
  summarize(count = n(), 
            avg.spend = mean(logprice)/mean(data.test$logprice)) %>% 
  arrange(desc(avg.spend))

table1=data.test %>% group_by(authorstandard) %>% 
  summarize(count = n(), avg.spend = mean(logprice)/mean(data.test$logprice)) %>% 
  filter(abs(avg.spend-1)>0.1) %>% 
  arrange(desc(avg.spend))

table2=data.test %>% group_by(authorstyle) %>% 
  summarize(count = n(), avg.spend = mean(logprice)/mean(data.test$logprice)) %>% 
  arrange(desc(avg.spend))

table3=data.test %>% group_by(artistliving) %>% 
  summarize(count = n(), avg.spend = mean(logprice)/mean(data.test$logprice)) %>% 
  arrange(desc(avg.spend))
#reduce factor levels for three varaibles
data.test$school_pntg[data.test$school_pntg %in% c("A","S","G","X")] <- "other or unknown" %>% as.factor
data.test$authorstandard<- ifelse(data.test$authorstandard  %in% table1$authorstandard, 1, 0)


# Selecting all the relevant variables for final table
data.test = data.test%>% 
  select( Shape, log.Surface, Height_in, Width_in, year, artistliving,
          diff_origin, lot,
          High_Roller, BigShotPainter, Nfigures, Material, position, endbuyer, 
          origin_author, winningbiddertype, type_intermed, dealer,
          authorstandard, school_pntg, dealer, origin_author, 
          engraved, original, prevcoll, othartist, paired, figures, diff_origin,
          lrgfont, relig, landsALL, lands_sc, lands_elem, lands_figs, lands_ment, 
          arch, mytho, peasant, othgenre, singlefig, portrait, still_life, discauth, 
          history, allegory, pastorale, other) %>% mutate(logprice = 0)
library(mice)

imputed_Data <- mice(data.test, m=10, maxit = 100, method = 'pmm', seed = 9)

completeData <- complete(imputed_Data,2)
completeData[which(is.na(data.test$log.Surface)),]$log.Surface = 
  mean(na.omit(data.test)$log.Surface, na.rm=TRUE)

# missing.data = which(apply(data.test, 1, function(x) max(is.na(x))==1))
# data.test = data.test%>% na.omit()

BMA.prediction = predict(basg, newdata = completeData, estimator = "BPM",
                         se.fit = TRUE, interval = "prediction")

preds = matrix(nrow = nrow(paintings_test), ncol = 3)
# preds[-missing.data,] = cbind(confint(BMA.prediction)[,3], 
#                                     confint(BMA.prediction)[,1], 
#                                     confint(BMA.prediction)[,2])
# 
# preds[missing.data,] = colMeans(cbind(confint(BMA.prediction)[,3],
#                                             confint(BMA.prediction)[,1], 
#                                             confint(BMA.prediction)[,2]))
preds = cbind(confint(BMA.prediction)[,3], confint(BMA.prediction)[,1], confint(BMA.prediction)[,2])

predictions = data.frame(fit = exp(preds[,1]), lwr = exp(preds[,2]), upr = exp(preds[,3]))


save(predictions, file="predict-test.Rdata")

```


---
title: "Part-II-Writeup"
author: "Brian Cozzi, Zhaolin Ying, Wei Zhang"
date: "12/8/2018"
output:
  pdf_document: default
  html_document: default
---

```{r read-data, echo=FALSE, message=FALSE, warning = FALSE}
load("paintings_train.Rdata")
load("paintings_test.Rdata")
load("paintings_validation.Rdata")
set.seed(9)

knitr::opts_chunk$set(echo = FALSE)
library(dplyr)
library(tidyr)
library(stringr)
library(ggplot2) 
library(glmnet)
library(knitr)
library(BAS)

```


### Part II: Write Up

Once you are satisfied with your model, provide a write up of your data analysis project in a new Rmd file/pdf file: `Part-II-Writeup.Rmd` by copying over salient parts of your R notebook and the previous writeup (you should also save the pdf version) The written assignment consists of five parts:

1. Introduction (1 point if improved from before)
  add previous intro with any edits
We are tasked with identifying the relevant variables for predicting the cost of a painting for a few years in the 18th century. In addition to gaining insight on these variables, we are also interested in generating a prediction to identify paintings that may have been overvalued or undervalued given a set of predictors. There are many factors that drive the prices of paintings. Our team wants to find the intrinsic value of paintings then we can judge whether each specific painting on the market has been overvalued or undervalued. In our project, we focus on the variable selection and tries to detect which variables tend to be informative.


Initial inspection of the data revealed that it contained 59 variables: 2 outcomes (price and logprice) and 56 predictors. These predictors fit into several relatively well-defined categories: \
- Artist Characteristics \
- Auction Characteristics \
- Painting Content and Physical Features  \

One of the challenges that these data present is that all of these features were categorical variables with varying levels of sparsity. That is, we may have very precise definitions of the characteristics surrounding the painting, however many of these values are sparsely populated and contain categories that are not well-populated. Therefore, one of our goals in the next seciton will be to identify these predictors and adjust them so they can be interpreted intuitively. 

Another major challenge is the redundancy of some variables. For instance, the variables describing the paintings physical features (height, diameter, surface, shape) can be reduced considerably. Therefore, another goal of subsequent sections will be to further reduce these data to improve interpretability and reduce the number of features that need to be used in the model. 


2. Exploratory data analysis (1 point if improved from before): 
   
#### Physical Features of Painting
```{r Physical Features, message=FALSE, echo=FALSE, warning = FALSE}
# Shape and Surface
plot1 = ggplot(data = paintings_train, 
               aes(x = Surface, y = logprice,
                   shape = school_pntg, color = Shape))+
                geom_point() + ggtitle("Surface and Log Price")

plot2 = ggplot (data = paintings_train, 
                aes(x = log(Surface), y = logprice,
                    shape = school_pntg, color = Shape))+
                geom_point() + ggtitle("Log Surface and Log Price") 
                
plot.df = paintings_train %>% mutate(lot = str_replace_all(lot, "lot", "")%>% as.numeric())
plot3 = ggplot (data = plot.df, 
                aes(x = lot, y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("Lot and Log Price")
plot4 = ggplot (data = plot.df, 
                aes(x = log(lot), y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("Log Lot and Log Price")

plot5 = ggplot (data = paintings_train, 
                aes(x = nfigures, y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("NFigures and Log Price")
plot6 = ggplot (data = plot.df, 
                aes(x = log(nfigures+1), y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("Log NFigures and Log Price")

gridExtra::grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, nrow = 3)


```

The plots above show several variables that become increasingly sparse as the values increase. In other words, their distributions are very positively skewed making them good candidates for a log transformation. The plots on the left show the relationship between the untransformed variables on the x axis and log price on the y axis and the plots on the right show the relationship of the log transformed values. Clearly, in all cases, the log transformation makes these plots more readable, though it seems that the only clear linear relationship is with log surface. For log lot, there does not appear to be a discernable relationship with price. Because these observations could significantly impact the intercept of this relationship, we determined that it would be best to simply transform this into a dichotomous variable with 0 signifiying 0 or 1 figures and 1 signifying more than 1 figures. 

We also see from these plots that the dealer, regardless of the variable on the x axis, seems to stratify the price of the painting fairly neatly. Additionally, we can see from the plot with surface that nearly all the paintings are squares or rectangles. 

#### Yearly Effects
```{r Sales by Dealer and Year, warning = FALSE, message = FALSE, echo=FALSE}

ggplot(data = paintings_train, aes(x = factor(year), y = logprice, color = dealer)) + 
  geom_jitter() + ggtitle("Sales by Dealer and Year") + xlab("Year")

```
The visualization above shows that the effect of hear on the median sale price does not appear to be linear. Without taking dealer into account yet, it seems that some years have very little activity and others are densely populated. From this visualization, it seems clear year should be interpreted as a factor given the information we have at our disposal. It is worth considering that the relative activity or inactivity may be due to other economic factors for which we do not have data. 

Another key takeaway from this is that some dealers were particularly active for some years, but relatively inactive for others. Without much subject area expertise, it is difficult to develop some intuition for why this may be happening. While this may initially suggest that an interaction term is useful, it should be noted that there are some years in which dealers are totally inactive which could seriously hinder the interpretability of the coefficients for the inactive years. 



### Data Processing
```{r, warning = FALSE, message = FALSE, echo=FALSE}
library(stringr)
library(tm)
library(reshape2)
library(tidyverse)
```


3. Discussion of preliminary model Part I (5 points)
Discuss performance based on leader board results and suggested refinements.

From the leader board results of Part I, we can see that our team (team 9) has the smallest bias. And our coverage is in the middle-to-high level.Our maxDeviation and meanabsdeviation are also very small compared to other groups. Our rmse value is in the middle level. So we think we should minimize rmse further in our Part II model. On the one hand, we try to include excluded variables like "subjects" to see if it can help to improve our model. On the other hand, We are considering using non-linear models like gradient boosting model, random forest and bart to see if it can make our predictions better.  



4.  Development of the final model (20 points)
* Final model: must include a summary table

* Variables: must include an explanation

There were several key changes to the variables that were used in this round because the original constraint of 20 variables was removed. 

In the original dataset, we created two new dichotomous variables from the names of the winning bidders and authors. For each variable, we looked at the top 10 names with respect to the average logprice over the average and coded the variable as 1 if they were on the list and 0 otherwise. In this case, rather than creating a variable that was either 0 or 1, we created a variable that was the name of the bidder/author if they were on the top 10 list or a placeholder value if they were not. Therefore, for each of these new factor variables had 11 unique values (10 distinct names and 1 placeholder value).

Our most significant improvement to this dataset came from the parsing of the subject description. In the original model, we removed the subject column entirely from our predition because it was a free text response. To make use of this variable, we believed that the text data had to be separated into its individual words which would then be able to predict the value of the painting. To do this, we first split each "subject" entry into its constituent words and removed any special characters or accent marks from those words. Once this was done, we filtered this list down to remove the "stop words" (e.g. and, then, the, were). This was particularly challenging because the descriptions were all written in French, so the R package "tm" had to be used to remove these words. These data were then transposed so each remaining word became its own column that was coded as 1 for each observation that contained the word and 0 for each observation that didn't. This was then further filtered using colmeans to remove any words that occurred in less than .5% of the subject descriptions. 

Finally, because there were missing data, we used the R package MICE to fill in these missing values. It is worth noting that this was not done for the training values, but only for the validation values to ensure consistency in the model build. 

```{r Format Data, include = FALSE, warning = FALSE, message = FALSE}
# Formatting for some variables
# Top 10 bidders

data = paintings_train
HighRollers = data %>% group_by(winningbidder) %>% 
              summarize(count = n(), avg.spend = mean(logprice)/mean(data$logprice)) %>% 
              filter(count>10) %>% arrange(desc(avg.spend)) %>% head(10) %>%
              select(winningbidder) %>%
              head(10) %>% as.list

# Top 5 Pricey Painters
BigShotPainters = data %>% group_by(authorstandard) %>% 
              summarize(count = n(), avg.haul = mean(logprice)/mean(data$logprice)) %>% 
              filter(count>10) %>% arrange(desc(avg.haul)) %>%
              select(authorstandard) %>%
              head(10) %>% as.list

format.data = function(paintings_train){
  data.train = paintings_train %>% 
        mutate( # Painting physical features
             Shape = ifelse(Shape=="squ_rect", 1, 0),
             log.Surface = log(Surface),
             Height_in = coalesce(Height_in, Diam_in),
             Width_in = coalesce(Width_in, Diam_in),
             year = factor(year),
             Nfigures = log(nfigures+1),
             # Nfigures = ifelse(nfigures>1, 1, 0),
             # Material = as.factor(materialCat),
             Material = material %in% c("cuivre", "argent"),
             position = ifelse(position>1, 1, position),
             # Buyer Characteristics
             High_Roller = ifelse(winningbidder %in% 
                                    unlist(HighRollers), 1, 0),
             High_Roller.name = ifelse(winningbidder %in% 
                                    unlist(HighRollers), winningbidder, "NA"),
             BigShotPainter = ifelse(authorstandard %in% 
                                    unlist(BigShotPainters), 1, 0),
             BigShotPainter.name = ifelse(authorstandard %in% 
                                    unlist(BigShotPainters), authorstandard, "NA"),
             endbuyer = ifelse(endbuyer=="U"|winningbiddertype == "",
                                        "N",
                                        winningbiddertype) %>% 
                                    substr(.,1,1) %>% as.factor(), 
             dealer = factor(dealer),
             lot = str_replace_all(lot, "lot", "")%>% as.numeric(),
             origin_author = factor(origin_author),
             winningbiddertype = ifelse(is.na(winningbiddertype)|winningbiddertype == "",
                                        "None",
                                        substr(winningbiddertype,1,1)) %>% as.factor(),
             type_intermed = factor(ifelse(type_intermed=="", 
                                           "None",
                                           type_intermed))%>% substr(.,1,1),
             school_pntg= ifelse(school_pntg %in% c("A","S","G","X"),
                                 "other or unknown", school_pntg) %>% 
                                  as.factor(),
             # authorstandard=as.factor(authorstandard), 
             artistliving=as.factor(artistliving),
             authorstyle=ifelse(authorstyle %in% c("m","g","o","al","co","pa"), 
                                "other", ifelse(authorstyle=="", "missing", 
                                                authorstyle)) %>% as.factor()
  ) 
  # Selecting all the relevant variables for final table
  data.train = data.train%>% 
    select(logprice, Shape, log.Surface, Height_in, Width_in, year, artistliving,
            lot, position, dealer, origin_author, origin_cat, school_pntg,
            diff_origin, authorstyle, Material, High_Roller, High_Roller.name, 
           BigShotPainter, BigShotPainter.name, #authorstandard,
            author, winningbiddertype, endbuyer, Interm, type_intermed,
             engraved, original, prevcoll, othartist, paired, figures, finished,
            lrgfont, subject,
            ## Painting Features
            relig, landsALL, lands_sc, lands_elem, lands_figs, lands_ment, arch,
            mytho, peasant, othgenre, singlefig, portrait, still_life, discauth, history,
            allegory, pastorale, other )
  
  return(data.train)
}
data.train = format.data(paintings_train)
data.test = format.data(paintings_test)
data.val = format.data(paintings_validation)

# paintings_train%>% group_by(substr(winningbiddertype,1,1)) %>% summarize(avg = mean(logprice)/mean(paintings_train$logprice), count = n()) %>% arrange(desc(avg))

```



```{r Formatting Subject, echo = FALSE, warning = FALSE, message = FALSE}
library(dplyr)
library(stringr)
library(tm)
library(reshape2)

format.subject = function(data.train){
  ## Fixing the subject to have multiple columns
  data.train.desc = data.train %>% select(subject) %>%
    mutate(subject = str_replace_all(subject, "\\(+[0-9]+\\)", "")) %>%
    mutate(subject = str_replace_all(subject, "\\,", "")) %>%
    mutate(subject = tolower(subject)) %>%
    mutate(subject = str_split(subject, " "), ID = row_number()) %>% unnest() %>%
    mutate(subject = str_replace_all(subject, "\\/", "")) %>% group_by(ID, subject) %>%
    filter(subject != "") %>% anti_join(data_frame(subject = stopwords(kind="french"))) 
  
  # print(dim(data.train.desc))
  ## Taking a look at the top words
  # 
  
  ## Adding each word to the dataset
  # Creating a dataset where each word is a column
  data.train.subject = data.train.desc %>% ungroup() %>% 
    mutate(subject = str_replace_all(subject, "[0-9]", "")) %>%
    mutate(subject = str_replace_all(subject, "\\+", "")) %>%
    mutate(subject = str_replace_all(subject, "\\=", "")) %>%
    mutate(subject = str_replace_all(subject,"[[:punct:]]", "")) %>%
    mutate(subject = iconv(subject, to="ASCII//TRANSLIT")) %>%
    mutate(subject = ifelse(subject == "", "None", subject)) %>%
    mutate(subject = factor(subject), val = n()) %>%
    group_by(ID,subject) %>%
    ungroup() %>% dcast(ID ~ subject, value.var="val")
  return(data.train.subject)
}
data.train.subject = format.subject(data.train)
data.test.subject = format.subject(data.test)
data.val.subject = format.subject(data.val)

# Find columns common to both
cols = data_frame(col = colnames(data.train.subject)) %>% inner_join(data_frame(col = colnames(data.val.subject))) 

# Some of these words are only used once... let's set the threshold to .5%
keepers = data_frame(col = colnames(data.train.subject)[which(apply(data.train.subject,2,mean)>.005)]) %>% inner_join(cols) %>% as.list()

data.train.features = data.train %>% select(logprice, relig, landsALL, lands_sc, lands_elem, lands_figs, 
                                            lands_ment, arch,mytho, peasant, othgenre, singlefig, 
                                            portrait, still_life, discauth, history,allegory, 
                                            pastorale, other) %>% mutate(ID = row_number()) %>%
  inner_join(data.train.subject[,keepers$col], by = "ID") #%>% select(-ID)

pred.data = data.train %>% select(Shape, log.Surface, Height_in, Width_in, year, artistliving,
          lot, position, dealer, origin_author, origin_cat, school_pntg,
          diff_origin, authorstyle, Material, High_Roller, High_Roller.name, 
         BigShotPainter, BigShotPainter.name, #authorstandard,author,
           winningbiddertype, endbuyer, type_intermed, #Interm,
           engraved, original, prevcoll, othartist, paired, figures, finished,
          lrgfont) %>% mutate(ID = row_number()) %>% #filter(ID %in% IDs)
  inner_join(data.train.features, by = "ID") %>% na.omit() %>% select(-ID)

pred.data = unclass(pred.data) %>% as.data.frame()

data.val.features = data.val %>% select(logprice, relig, landsALL, lands_sc, lands_elem, lands_figs, 
                                            lands_ment, arch,mytho, peasant, othgenre, singlefig, 
                                            portrait, still_life, discauth, history,allegory, 
                                            pastorale, other) %>% mutate(ID = row_number()) %>%
  inner_join(data.val.subject[,keepers$col], by = "ID") # %>% select(-ID)

pred.data.val = data.val %>% select(Shape, log.Surface, Height_in, Width_in, year, artistliving,
          lot, position, dealer, origin_author, origin_cat, school_pntg,
          diff_origin, authorstyle, Material, High_Roller, High_Roller.name, 
         BigShotPainter, BigShotPainter.name, #authorstandard,author,
           winningbiddertype, endbuyer, type_intermed, #Interm,
           engraved, original, prevcoll, othartist, paired, figures, finished,
          lrgfont) %>% mutate(ID = row_number()) %>% #filter(ID %in% IDs)
  left_join(data.val.features, by = "ID") %>% select(-ID) # removing na.omit

pred.data.val = unclass(pred.data.val) %>% as.data.frame()


```




* Variable selection/shrinkage: must use appropriate method and include an explanation
In terms of variable selection we used BMA which, because it is an average of many models, did not technically reduce the number of variables we used. However, using this approach, we were able to take into account the posterior marginal inclusion probabilities. Keeping only variables with postierior inclusion probabilities greater than .2, we see there are considerably fewer variables than the 89 we originally started with. Among these, we notice that several of the subject description variables are present.

The plots below show some exploratory work on the data reduction. The first set of plots show the attempted reduction with MCA and the second plot shows the top words by price above average. 


```{r Feature Reduction, echo = FALSE, warning = FALSE, message = FALSE}
library(PCAmixdata)
PCA.df = lapply(data.train.features%>%select(-logprice, -ID), function(x) as.factor(x)) %>% as.data.frame()

PCA = PCAmix(X.quali = PCA.df, ndim = 10, rename.level = TRUE,
  weight.col.quanti = NULL, weight.col.quali = NULL, graph = TRUE)

PCA.scores = as.data.frame(PCA$scores.stand) %>% mutate(ID = row_number())
colnames(PCA.scores) = str_replace_all(colnames(PCA.scores), " ", "")

par(mfrow=c(2,2))
plot(PCA)

# Most important PCs
word.selection = step(lm(logprice~., data = data.train.features%>%select(-ID)), trace = FALSE)
summary(word.selection)
# data.train %>% filter(data.train.features$)
final.words = names(word.selection$coefficients)[-1]

data.train.features.reduced = data.train.features[,c(final.words, "ID", "logprice")]
data.val.features.reduced = data.val.features[,c(final.words, "ID", "logprice")]

pred.data = data.train %>% select(Shape, log.Surface, Height_in, Width_in, year, artistliving,
          lot, position, dealer, origin_author, origin_cat, school_pntg,
          diff_origin, authorstyle, Material, High_Roller, High_Roller.name, 
         BigShotPainter, BigShotPainter.name, #authorstandard,author,
           winningbiddertype, endbuyer, type_intermed, #Interm,
           engraved, original, prevcoll, othartist, paired, figures, finished,
          lrgfont) %>% mutate(ID = row_number()) %>% #filter(ID %in% IDs)
  inner_join(data.train.features.reduced, by = "ID") %>% na.omit() %>% select(-ID)
pred.data = unclass(pred.data) %>% as.data.frame()

pred.data.val = data.val %>% select(Shape, log.Surface, Height_in, Width_in, year, artistliving,
          lot, position, dealer, origin_author, origin_cat, school_pntg,
          diff_origin, authorstyle, Material, High_Roller, High_Roller.name, 
         BigShotPainter, BigShotPainter.name, #authorstandard,author,
           winningbiddertype, endbuyer, type_intermed, #Interm,
           engraved, original, prevcoll, othartist, paired, figures, finished,
          lrgfont) %>% mutate(ID = row_number()) %>% #filter(ID %in% IDs)
  left_join(data.val.features.reduced, by = "ID") %>% select(-ID)
pred.data.val = unclass(pred.data.val) %>% as.data.frame()


```




```{r BMA, echo=FALSE, warning = FALSE, message = FALSE}
library(BAS)
rmse = function(est, obs) return(sqrt(mean((est-obs)^2)))
set.seed(123)
Fold = sample(rep(1:10, ceiling(nrow(pred.data)/5)), nrow(pred.data))
j=1
Train = pred.data[Fold!=j ,]
Test = pred.data[Fold==j ,] 
basg=bas.lm(logprice ~ . + lot:dealer + Shape:log.Surface + 
              position:dealer + artistliving:position
      , data=Train%>% select(-authorstyle, -High_Roller, -BigShotPainter, -winningbiddertype),
      prior="g-prior", a=nrow(Train), modelprior=uniform(), 
      method="MCMC", MCMC.iterations = 500000, thin = 20)
BMA <- predict(basg, estimator = "BMA")

BMA.prediction = predict(basg, newdata = Test, estimator = "BMA", 
                         se.fit = TRUE, interval = "prediction")
rmse(exp(confint(BMA.prediction)[,3]), exp(Test$logprice))

x = confint(coef(basg, estimator = "BMA"))

Plot.df = data_frame(Variable = attr(x, "dimnames")[[1]], 
                  Estimate = x[,3], Lower = x[,1], 
                  Upper = x[,2]) %>%
  mutate(In.Zero = ifelse(Upper*Lower<0, 1, 0)) %>%
  filter(basg$probne0>.2)
Plot.df %>% arrange(desc(Estimate))

```

```{r, echo=FALSE, warning = FALSE, message = FALSE}
ggplot(data = Plot.df, aes(x = Variable, y = Estimate, color = factor(In.Zero)))+ 
  geom_point() +
  geom_errorbar(aes(ymin = Lower, ymax = Upper))+ 
  theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 8))

```


### Building BMA with remaining features

* Residuals and Model Evaluation
```{r Residual Plot, echo = FALSE, warning = FALSE, message = FALSE}
par(mfrow = c(2,2))
plot(basg)

```

From the residual plot in the top left we notice, as before, that the residuals appear to be normally distributed with mean 0. There does not to be any trend linear or otherwise. One notable plot from these diagnostics is the plot of marginal inclusion probabilities. Unlike our previous plot which showed either very high or very low marginal inclusion probabilities, this plot shows that there are some variables with values closer to .5 or, at the very least, further from 1 and 0. This may suggest some sort of collinearity between this new features which could be caused by the inclusion of some of the words from the subject description. 

* discussion of how prediction intervals obtained  \ 

Normally prediciton intervals for BMA are obtained through the confint procedure in the R package. According to the package description, it constructs approximate 95% Highest Posterior Density intervals using "nsim" draws from the mixture of Student t distributions are obtained with the HPD interval obtained for each of the Monte Carlo iterations.



5. Assessment of the final model (25 points)

* Model testing : must include a discussion \ 


Because of the nature of bayesian model averaging and because of run time limitations, we chose a random 90% subset which we could use to build the model and used the remaining 10% to test the model. With other models (such as stepwise AIC/BIC selection or LASSO/Ridge Shrinkage), it may be more useful to do k-fold cross validation which was our original intent before we settled on BMA for our final model. In this case, the use evenly partitions the dataset in to k groups and performs the model build with the k-1 groups and testing with the held out group. For our test set, we saw an RMSE of 1492. 



```{r Expensive Words}
ggplot(data.train %>% select(-subject) %>%
    mutate(ID = row_number()) %>% inner_join(data.train.desc) %>%
    group_by(subject) %>% summarize(avg.cost = mean(logprice)/mean(data.train$logprice), freq = n()) %>%
    filter(freq>length(data.train$logprice)*.015) %>% arrange(desc(avg.cost)) %>% head(10), aes(x = avg.cost, y = freq, color =subject)) + geom_label(aes(label = subject))


```


## Submit Validation Data Predictions
```{r BAS prediction, include = FALSE, echo = FALSE, warning = FALSE, message = FALSE}
library(mice)
imputed_Data <- mice(pred.data.val, m=2, maxit = 10, method = 'pmm', seed = 9)
test.imputation = complete(imputed_Data) 
# apply(test.imputation, 2, function(x) max(is.na(x)))

#imputing surface
# missing = test.imputation[is.na(test.imputation$log.Surface),]
# log.surface.lm = lm(log.Surface~., data = pred.data%>% select(-logprice))
test.imputation$log.Surface[is.na(test.imputation$log.Surface)] = mean(test.imputation$log.Surface, na.rm=TRUE)
# test.imputation[is.na(test.imputation$log.Surface), "log.Surface"] = predict(log.surface.lm, missing)


BMA.prediction = predict(basg, newdata = test.imputation, estimator = "BMA", 
                         se.fit = TRUE,
                         interval = "prediction")

# str(BMA.prediction)
# x = confint(BMA.prediction)
# preds = cbind(confint(BMA.prediction)[,3], confint(BMA.prediction)[,1], confint(BMA.prediction)[,2]) 

# predictions = data.frame(fit = exp(BMA.prediction$fit), lwr = exp(BMA.prediction$fit-2), upr = exp(BMA.prediction$fit+2))
predictions = data.frame(fit = exp(BMA.prediction$fit), 
                         lwr = exp(BMA.prediction$fit - 2*BMA.prediction$se.bma.pred), 
                         upr = exp(BMA.prediction$fit + 2*BMA.prediction$se.bma.pred))


save(predictions, file="predict-validation.Rdata")
save(predictions, file="prediction-validation.Rdata")


```


* Model result: must include a selection and discussion of the top 10 valued  paintings in the validation data.
```{r Top 10 Paintings, echo = FALSE, warning = FALSE, message = FALSE}
predictions %>% mutate(ID = row_number()) %>% inner_join(data.val %>% mutate(ID = row_number()), by= "ID") %>% arrange(desc(fit)) %>% select(subject, BigShotPainter.name, High_Roller.name, dealer, year, fit)%>% arrange(desc(fit)) %>% mutate(BigShotPainter.name = substr(iconv(BigShotPainter.name, to="ASCII//TRANSLIT"),1,10),
                                                High_Roller.name = substr(iconv(High_Roller.name, to="ASCII//TRANSLIT"),1,10),
                                                subject = substr(iconv(subject, to="UTF-8"),1,60))%>%
  head(10) %>% kable(., digits = 0, align = 'c', format = "markdown")
```

From these top paintings, we notice a few common themes. First, we see that the dealer is mostly "R" and 1777 is the most frequently occurring year. Interestingly, we notice that only 4 of the paintings do we have bidders (High Rollers) that tend to be associated with high-cost paintings. Finally, we notice that the subject contains some of the words we picked out from our earlier analysis. The words Paysages, Homme, and Femme show up in several of these subject descriptions. 


6. Conclusion (10 points): must include a summary of results and a discussion of things learned. Optional what would you do if you had more time.


Summary of result:
The results here were similar to what we found before: it seems that the content and the physical features of the painting were not nearly as important as the circumstances under which it was sold. Namely, the year and dealer had a much larger impact than any of the words that were mined from the subject description. As we noted early on in our first write up, it appears that the yearly effect and dealer effect appear to be correlated. An important finding is that not only are these two variables related, but it appears that the actual content of the painting was also related to the painter, the dealer, and the year in which it was sold. Therefore, it is even more difficult to discern the relationships between variables because they all seem to be, to at least some degree, dependent on one another. 

To that end, it would be very useful to do some form of variable reduction on these data. In an earlier iteration, we looked to reduce the number of physical features we used from the subject description by performing MCA on these features. This was met with limited success, though it stands to reason that more sophisticated clustering or grouping techniques could be done to reduce the collinearity of these features (though it may sacrifice the model's interpretability). 





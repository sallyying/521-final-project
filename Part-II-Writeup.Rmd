---
title: "Part-II-Writeup"
author: "Brian Cozzi"
date: "12/8/2018"
output: html_document
---


### Part II: Write Up

Once you are satisfied with your model, provide a write up of your data analysis project in a new Rmd file/pdf file: `Part-II-Writeup.Rmd` by copying over salient parts of your R notebook and the previous writeup (you should also save the pdf version) The written assignment consists of five parts:

1. Introduction (1 point if improved from before)
  add previous intro with any edits
We are tasked with identifying the relevant variables for predicting the cost of a painting for a few years in the 18th century. In addition to gaining insight on these variables, we are also interested in generating a prediction to identify paintings that may have been overvalued or undervalued given a set of predictors. There are many factors that drive the prices of paintings. Our team wants to find the intrinsic value of paintings then we can judge whether each specific painting on the market has been overvalued or undervalued. In our project, we focus on the variable selection and tries to detect which variables tend to be informative.


Initial inspection of the data revealed that it contained 59 variables: 2 outcomes (price and logprice) and 56 predictors. These predictors fit into several relatively well-defined categories: \
- Artist Characteristics \
- Auction Characteristics \
- Painting Content and Physical Features  \

One of the challenges that these data present is that all of these features were categorical variables with varying levels of sparsity. That is, we may have very precise definitions of the characteristics surrounding the painting, however many of these values are sparsely populated and contain categories that are not well-populated. Therefore, one of our goals in the next seciton will be to identify these predictors and adjust them so they can be interpreted intuitively. 

Another major challenge is the redundancy of some variables. For instance, the variables describing the paintings physical features (height, diameter, surface, shape) can be reduced considerably. Therefore, another goal of subsequent sections will be to further reduce these data to improve interpretability and reduce the number of features that need to be used in the model. 


2. Exploratory data analysis (1 point if improved from before): 
   add previous EDA
   
   
#### Physical Features of Painting
```{r Physical Features, message=FALSE, echo=FALSE}
# Shape and Surface
plot1 = ggplot(data = paintings_train, 
               aes(x = Surface, y = logprice,
                   shape = school_pntg, color = Shape))+
                geom_point() + ggtitle("Surface and Log Price")

plot2 = ggplot (data = paintings_train, 
                aes(x = log(Surface), y = logprice,
                    shape = school_pntg, color = Shape))+
                geom_point() + ggtitle("Log Surface and Log Price") 
                
plot.df = paintings_train %>% mutate(lot = str_replace_all(lot, "lot", "")%>% as.numeric())
plot3 = ggplot (data = plot.df, 
                aes(x = lot, y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("Lot and Log Price")
plot4 = ggplot (data = plot.df, 
                aes(x = log(lot), y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("Log Lot and Log Price")

plot5 = ggplot (data = paintings_train, 
                aes(x = nfigures, y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("NFigures and Log Price")
plot6 = ggplot (data = plot.df, 
                aes(x = log(nfigures+1), y = logprice, color = factor(dealer)))+
                geom_point() + ggtitle("Log NFigures and Log Price")

gridExtra::grid.arrange(plot1, plot2, plot3, plot4, plot5, plot6, nrow = 3)


```

The plots above show several variables that become increasingly sparse as the values increase. In other words, their distributions are very positively skewed making them good candidates for a log transformation. The plots on the left show the relationship between the untransformed variables on the x axis and log price on the y axis and the plots on the right show the relationship of the log transformed values. Clearly, in all cases, the log transformation makes these plots more readable, though it seems that the only clear linear relationship is with log surface. For log lot, there does not appear to be a discernable relationship with price. With NFigures, we notice there are a lot of . Because these observations could significantly impact the intercept of this relationship, we determined that it would be best to simply transform this into a dichotomous variable with 0 signifiying 0 or 1 figures and 1 signifying more than 1 figures. 

We also see from these plots that the dealer, regardless of the variable on the x axis, seems to stratify the price of the painting fairly neatly. Additionally, we can see from the plot with surface that nearly all the paintings are squares or rectangles. 

#### Yearly Effects
```{r Sales by Dealer and Year, echo=FALSE}

ggplot(data = paintings_train, aes(x = factor(year), y = logprice, color = dealer)) + 
  geom_jitter() + ggtitle("Sales by Dealer and Year") + xlab("Year")

```
The visualization above shows that the effect of hear on the median sale price does not appear to be linear. Without taking dealer into account yet, it seems that some years have very little activity and others are densely populated. From this visualization, it seems clear year should be interpreted as a factor given the information we have at our disposal. It is worth considering that the relative activity or inactivity may be due to other economic factors for which we do not have data. 

Another key takeaway from this is that some dealers were particularly active for some years, but relatively inactive for others. Without much subject area expertise, it is difficult to develop some intuition for why this may be happening. While this may initially suggest that an interaction term is useful, it should be noted that there are some years in which dealers are totally inactive which could seriously hinder the interpretability of the coefficients for the inactive years. 



### Data Processing
```{r}
library(dplyr)
library(stringr)
library(tm)
library(reshape2)
library(tidyverse)
```


3. Discussion of preliminary model Part I (5 points)
Discuss performance based on leader board results and suggested refinements.






4.  Development of the final model (20 points)
* Final model: must include a summary table

* Variables: must include an explanation

* Variable selection/shrinkage: must use appropriate method and include an explanation

* Residual: must include a residual plot and a discussion

* discussion of how prediction intervals obtained 

```{r Format Data}
# Formatting for some variables
# Top 10 bidders
data = paintings_train
HighRollers = data %>% group_by(winningbidder) %>% 
              summarize(count = n(), avg.spend = mean(logprice)/mean(data$logprice)) %>% 
              filter(count>10) %>% arrange(desc(avg.spend)) %>% head(10) %>%
              select(winningbidder) %>%
              head(10) %>% as.list

# Top 5 Pricey Painters
BigShotPainters = data %>% group_by(authorstandard) %>% 
              summarize(count = n(), avg.haul = mean(logprice)/mean(data$logprice)) %>% 
              filter(count>10) %>% arrange(desc(avg.haul)) %>%
              select(authorstandard) %>%
              head(10) %>% as.list

## Train data
data.train = paintings_train %>% 
      mutate( # Painting physical features
           Shape = ifelse(Shape=="squ_rect", 1, 0),
           log.Surface = log(Surface),
           Height_in = coalesce(Height_in, Diam_in),
           Width_in = coalesce(Width_in, Diam_in),
           year = factor(year),
           Nfigures = ifelse(nfigures>1, 1, 0),
           Material = material %in% c("cuivre", "argent"),
           position = ifelse(position>1, 1, position),
           # Buyer Characteristics
           High_Roller = ifelse(winningbidder %in% 
                                  unlist(HighRollers), 1, 0),
           High_Roller.name = ifelse(winningbidder %in% 
                                  unlist(HighRollers), winningbidder, "Cheap"),
           BigShotPainter = ifelse(authorstandard %in% 
                                  unlist(BigShotPainters), 1, 0),
           BigShotPainter.name = ifelse(authorstandard %in% 
                                  unlist(BigShotPainters), authorstandard, "Hack"),
           endbuyer = ifelse(endbuyer=="U"|winningbiddertype == "",
                                      "N",
                                      winningbiddertype) %>% 
                                  substr(.,1,1) %>% as.factor(), 
           dealer = factor(dealer),
           lot = str_replace_all(lot, "lot", "")%>% as.numeric(),
           origin_author = factor(origin_author),
           winningbiddertype = ifelse(is.na(winningbiddertype)|winningbiddertype == "",
                                      "None",
                                      winningbiddertype) %>% as.factor(),
           type_intermed = factor(ifelse(type_intermed=="", 
                                         "None",
                                         type_intermed))%>% substr(.,1,1),
           school_pntg= ifelse(school_pntg %in% c("A","S","G","X"),
                               "other or unknown", school_pntg) %>% 
                                as.factor(),
           # authorstandard=as.factor(authorstandard), 
           artistliving=as.factor(artistliving),
           authorstyle=ifelse(authorstyle %in% c("m","g","o","al","co","pa"), 
                              "other", ifelse(authorstyle=="", "missing", 
                                              authorstyle)) %>% as.factor()
) 
# Selecting all the relevant variables for final table
data.train = data.train%>% 
  select(logprice, Shape, log.Surface, Height_in, Width_in, year, artistliving,
          lot, position, dealer, origin_author, origin_cat, school_pntg,
          diff_origin, authorstyle, Material, High_Roller, High_Roller.name, 
         BigShotPainter, BigShotPainter.name, #authorstandard,
          author, winningbiddertype, endbuyer, Interm, type_intermed,
           engraved, original, prevcoll, othartist, paired, figures, finished,
          lrgfont, subject,
          ## Painting Features
          relig, landsALL, lands_sc, lands_elem, lands_figs, lands_ment, arch,
          mytho, peasant, othgenre, singlefig, portrait, still_life, discauth, history,
          allegory, pastorale, other )

```

```{r Formatting Subject}
library(dplyr)
library(stringr)
library(tm)
library(reshape2)

## Fixing the subject to have multiple columns
data.train.desc = data.train %>% select(subject) %>%
  mutate(subject = str_replace_all(subject, "\\(+[0-9]+\\)", "")) %>%
  mutate(subject = str_replace_all(subject, "\\,", "")) %>%
  mutate(subject = tolower(subject)) %>%
  mutate(subject = str_split(subject, " "), ID = row_number()) %>% unnest() %>%
  mutate(subject = str_replace_all(subject, "\\/", "")) %>% group_by(ID, subject) %>%
  filter(subject != "") %>% anti_join(data_frame(subject = stopwords(kind="french"))) 

## Taking a look at the top words
data.train %>% select(-subject) %>%
  mutate(ID = row_number()) %>% inner_join(data.train.desc) %>%
  group_by(subject) %>% summarize(avg.cost = mean(logprice), freq = n()) %>%
  filter(freq>10) %>% arrange(desc(avg.cost))

## Adding each word to the dataset
# Creating a dataset where each word is a column
data.train.subject = data.train.desc %>% ungroup() %>% 
  mutate(subject = str_replace_all(subject, "[0-9]", "")) %>%
  mutate(subject = str_replace_all(subject, "\\+", "")) %>%
  mutate(subject = str_replace_all(subject, "\\=", "")) %>%
  mutate(subject = str_replace_all(subject,"[[:punct:]]", "")) %>%
  mutate(subject = iconv(subject, to="ASCII//TRANSLIT")) %>%
  filter(subject != "") %>%
  mutate(subject = factor(subject), val = n()) %>%
  group_by(ID,subject) %>%
  ungroup() %>% dcast(ID ~ subject, value.var="val")

# Some of these words are only used once... let's set the threshold to .5%
keepers = which(apply(data.train.subject,2,mean)>.005)

data.train.features = data.train %>% select(logprice, relig, landsALL, lands_sc, lands_elem, lands_figs, 
                                            lands_ment, arch,mytho, peasant, othgenre, singlefig, 
                                            portrait, still_life, discauth, history,allegory, 
                                            pastorale, other) %>% mutate(ID = row_number()) %>%
  inner_join(data.train.subject[,keepers], by = "ID") %>% select(-ID)



```

```{r MixedPCA}
# install.packages('PCAmixdata')
library(PCAmixdata)
PCA.df = lapply(data.train.features%>%select(-logprice), function(x) as.factor(x)) %>% as.data.frame()

PCA = PCAmix(X.quali = PCA.df, ndim = 15, rename.level = TRUE,
  weight.col.quanti = NULL, weight.col.quali = NULL, graph = TRUE)

PCA.scores = as.data.frame(PCA$scores.stand) %>% mutate(ID = row_number())
colnames(PCA.scores) = str_replace_all(colnames(PCA.scores), " ", "")

```

```{r 3D Scatterplot with 2 PCs}
library(scatterplot3d)

dims = data_frame(Dim1 = PCA$scores.stand[,1], Dim2 = PCA$scores.stand[,13], logprice = data.train.features[,1]) %>% filter(Dim1<10 & Dim2<10)

# 3D Scatter
scatterplot3d(x = dims$Dim1, y = dims$Dim2, z = dims$logprice, pch = 16, angle = 200)

library(plotly)
p <- plot_ly(dims, x = ~Dim1, y = ~Dim2, z = ~logprice) %>%
  add_markers() %>%
  layout(scene = list(xaxis = list(title = 'Dim1'),
                     yaxis = list(title = 'Dim2'),
                     zaxis = list(title = 'LogPrice')))

p
```

   
















```{r Random Forest - Reduced Data}
# Create Dataset
RMSE = function(est, obs) return(sqrt(mean((est-obs)^2)))

reduced.forest.data = as_tibble(PCA$scores.stand) %>% mutate(logprice = data.train.features$logprice)
colnames(reduced.forest.data) = str_replace_all(colnames(reduced.forest.data), " ", "")

reduced.forest = randomForest(logprice~., data = reduced.forest.data)  # Seeing how this does with the whole dataset
varImpPlot(reduced.forest)

predictions = data_frame(logprice = data.train.features$logprice, full.pred = predict(test.forest, data.train.features), reduced.pred = predict(reduced.forest, reduced.forest.data))

ggplot(predictions, aes(x = reduced.pred-logprice, y = full.pred-logprice)) + geom_point() + geom_abline(slope = 1, intercept = 0)

RMSE(exp(predictions$reduced.pred), exp(predictions$logprice))
RMSE(exp(predictions$full.pred), exp(predictions$logprice))

```


```{r}
library(bark)
library(e1071)
library(BART)
library(randomForest)
library(xgboost)
```


# Trying out a few algorithms
### Set train and test set
```{r}
data.train.features2 = data.train %>% select(logprice, relig, landsALL, lands_sc, lands_elem, lands_figs, 
                                            lands_ment, arch,mytho, peasant, othgenre, singlefig, 
                                            portrait, still_life, discauth, history,allegory, 
                                            pastorale, other) %>% mutate(ID = row_number()) %>%
  inner_join(data.train.subject[,keepers], by = "ID")
# IDs = unique(data.train.features2$ID)
# data.train.features2 = data.train.features2%>% select(-ID)

pred.data = data.train %>% select(Shape, log.Surface, Height_in, Width_in, year, artistliving,
          lot, position, dealer, origin_author, origin_cat, school_pntg,
          diff_origin, authorstyle, Material, High_Roller, High_Roller.name, 
         BigShotPainter, BigShotPainter.name, #authorstandard,author,
           winningbiddertype, endbuyer, type_intermed, #Interm,
           engraved, original, prevcoll, othartist, paired, figures, finished,
          lrgfont) %>% mutate(ID = row_number()) %>% #filter(ID %in% IDs)
  inner_join(data.train.features2, by = "ID") %>% na.omit() %>% select(-ID)

# pred.data = lapply(pred.data, function(x) as.factor(x)) %>% as.data.frame()
# pred.data[,c("logSurface", "logprice", "Height_in", "Width_in", "lot", "position")] = 
#   lapply(pred.data[,c("logSurface", "logprice", "Height_in", "Width_in", "lot", "position")], function(x) as.numeric(x))

pred.data = unclass(pred.data) %>% as.data.frame()

# colnames(data.train.features2)
# missing = apply(pred.data, 2, function(x) sum(is.na(x)))
train = sample(nrow(pred.data), nrow(pred.data)*.7)
pred.data.train = pred.data[train,]
pred.data.test = pred.data[-train,]

```

```{r}
test.forest = randomForest(logprice~., data = data.train.features)  # Seeing how this does with the whole dataset

pred.model.mat = model.matrix(logprice~., pred.data)
colnames(pred.data) = c(colnames(pred.data)[1:48], col.keys$key)

barksample = bark(as.matrix(pred.data[train, -1]), pred.data[train, 1], 
                              x.test = as.matrix(pred.data[-train, -1]),
                    classification = FALSE,
                    type="se")

apply(paintings_train, 2, class)

```
## Random Forest
```{r}
reduced.forest = randomForest(logprice~., data = pred.data.train)  # Seeing how this does with the whole dataset
varImpPlot(reduced.forest)

predictions = data_frame(logprice = as.numeric(pred.data.test$logprice), full.pred = predict(reduced.forest, pred.data.test))

# RMSE(exp(predictions$reduced.pred), exp(predictions$logprice))
RMSE(exp(predictions$full.pred), exp(predictions$logprice))

predictions$full.pred

lm(logprice~., data = test.data)
test = step(lm(logprice~., data = data.train.features2))
summary(test)

lm(logprice~., data = pred.data)
apply(pred.data, 2, class)
str(pred.data)
str(data.train.features2)

```


```{r Imputing Missing Data}





```



5. Assessment of the final model (25 points)

* Model evaluation: must include an evaluation discussion

* Model testing : must include a discussion
### Cross Validation

* Model result: must include a selection and discussion of the top 10 valued  paintings in the validation data.


6. Conclusion (10 points): must include a summary of results and a discussion of things learned. Optional what would you do if you had more time.



